{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15376367-0287-4618-ad82-03ff0f70ca20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain langgraph databricks-sdk databricks-vectorsearch\n",
    "%pip install databricks-sdk[openai]\n",
    "%pip install grandalf\n",
    "%pip install pyppeteer\n",
    "%pip install -U databricks-agents>=0.16.0 mlflow>=2.20.2 databricks-langchain databricks-openai\n",
    "%pip install --upgrade \"mlflow-skinny[databricks]\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42798886-ad4c-452a-8ddd-1ffe4ff3acf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e371dd-ba51-471b-b803-6d178204445a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Predictive Maintenance Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1cbb8e7-34d2-46d6-8acf-86c4134ff3a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile chat_agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain_core.tools import tool\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_community.chat_models.databricks import ChatDatabricks\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from mlflow.langchain.chat_agent_langgraph import parse_message\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Configure\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"genai_demo\"\n",
    "\n",
    "# Anomaly Detection Model\n",
    "MODEL_NAME = \"isolation_forest_pm_model\"\n",
    "MODEL_NAME_FULL = f\"{CATALOG}.{SCHEMA}.{MODEL_NAME  }\"\n",
    "MODEL_VERSION = 1\n",
    "MODEL_URI = f'models:/{MODEL_NAME_FULL}/{MODEL_VERSION}'\n",
    "\n",
    "# Vector Index\n",
    "INDEX_NAME = \"maintenance_docs_index\"\n",
    "INDEX_NAME_FULL = f\"{CATALOG}.{SCHEMA}.{INDEX_NAME}\"\n",
    "\n",
    "# LLM\n",
    "LLM_MODEL = \"gpt-41\"\n",
    "TEMPERATURE = 0.1\n",
    "# LLM_MODEL = \"databricks-llama-4-maverick\"\n",
    "\n",
    "\n",
    "# Load resources: model, retriever, LLM\n",
    "ad_model = mlflow.sklearn.load_model(MODEL_URI)\n",
    "\n",
    "vsc = VectorSearchClient()\n",
    "index = vsc.get_index(index_name=INDEX_NAME_FULL)\n",
    "\n",
    "# ws = WorkspaceClient()\n",
    "# chat_client = ws.serving_endpoints.get_open_ai_client()\n",
    "llm = ChatDatabricks(\n",
    "    target_uri=\"databricks\",\n",
    "    endpoint=LLM_MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    ")\n",
    "\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def anomaly_detector(vibration: float, pressure: float, temperature: float) -> str:\n",
    "    \"\"\"\n",
    "    Detects anomalies in equipment behavior using vibration, pressure, and temperature.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prediction = ad_model.predict([[vibration, pressure, temperature]])\n",
    "        result = \"Anomalous\" if prediction[0] == -1 else \"Normal\"\n",
    "        return f\"Anomaly Detection Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "    \n",
    "\n",
    "@tool\n",
    "def vector_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the vector index for machine manual documents.\"\"\"\n",
    "    try:\n",
    "        # Search the index with the query string\n",
    "        res = index.similarity_search(\n",
    "            query_text=query,\n",
    "            columns=[\"chunk_text\"],\n",
    "            num_results=1,\n",
    "            query_type=\"hybrid\"\n",
    "            )\n",
    "        context = \"\\n\\n\".join([r[0] for r in res[\"result\"][\"data_array\"]])\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        return f\"Vector search error: {str(e)}\"\n",
    "\n",
    "\n",
    "tools = [anomaly_detector, vector_search]\n",
    "\n",
    "# Define Nodes\n",
    "system_prompt = SystemMessage(\n",
    "    content=(\n",
    "        \"You are a predictive maintenance engineer. Answer machine maintenance queries using the search index. \"\n",
    "        \"If sensor data is provided, use the anomaly detection tool. \"\n",
    "        \"If the machine is anomalous, ask user whether RCA and resolution is required if user does not suggest anything otherwise continue the task.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Add memory\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "def create_agent(llm, tools, system_prompt):\n",
    "\n",
    "    class AgentState(TypedDict):\n",
    "        messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # assistant node\n",
    "    def assistant_node(state: AgentState) -> AgentState:\n",
    "        msgs = state[\"messages\"]\n",
    "        # Prepend system prompt if first turn\n",
    "        if not any(isinstance(m, SystemMessage) for m in msgs):\n",
    "            msgs = [system_prompt] + msgs\n",
    "\n",
    "        response = llm_with_tools.invoke(msgs)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    # Tools node\n",
    "    tools_node = ToolNode(tools)\n",
    "\n",
    "    # Build graph\n",
    "    builder = StateGraph(AgentState)\n",
    "    builder.add_node(\"assistant\", assistant_node)\n",
    "    builder.add_node(\"tools\", tools_node)\n",
    "\n",
    "    builder.add_edge(START, \"assistant\")\n",
    "    builder.add_conditional_edges(\n",
    "        \"assistant\",\n",
    "        tools_condition\n",
    "    )\n",
    "    builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "    # agent = builder.compile(checkpointer=checkpointer)\n",
    "    agent = builder.compile()\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self, \n",
    "        messages: list[ChatAgentMessage], \n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        res = self.agent.invoke(request)\n",
    "        response = [ChatAgentMessage(**parse_message(r)) for r in res[\"messages\"]]\n",
    "        return ChatAgentResponse(messages=response)\n",
    "    \n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                for m in node_data.get(\"messages\", []):\n",
    "                    msg = parse_message(m)\n",
    "                    yield ChatAgentChunk(delta=ChatAgentMessage(**msg))\n",
    "\n",
    "\n",
    "pm_agent = create_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(pm_agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0466895d-137c-4f56-916d-1f0ea38f6b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.langchain.chat_agent_langgraph import parse_message\n",
    "parse_message(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b7398b-b5b4-4986-9d95-30a920228bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parse_message(node_data[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33c8b64-31d1-4bc4-bb57-0221ba249f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(hist_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec29e57-a59b-4c00-a463-7a3461b9abc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage,AIMessage\n",
    "from chat_agent import pm_agent\n",
    "messages = [\n",
    "    # HumanMessage(content=\"hello\"),\n",
    "    # AIMessage(content=\"hello, how can I assist you?\"),\n",
    "    # HumanMessage(content=\"My machine is running very hot, I am wondering what the cuase is and how I can solve it.\")\n",
    "    {\"role\": \"user\", \"content\": \"hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"hello, how can I assist you?\"},\n",
    "    {\"role\": \"user\", \"content\": \"My machine is running very hot, could you help check the manual what could be cause and how I can fix it?\"}\n",
    "]\n",
    "request = {\"messages\": messages}\n",
    "# response = pm_agent.invoke(request)\n",
    "# for m in response['messages']:\n",
    "    # m.pretty_print()\n",
    "\n",
    "hist_lc = []\n",
    "for chunk in pm_agent.stream(request):\n",
    "    # print(chunk)\n",
    "    hist_lc.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390d2a0c-c011-42f1-8169-035547e46e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# messages = [HumanMessage(content=\"The machine's vibration is 3.2, pressure is 45, temperature is 78. If the machine is anomalous, please do a detail RCA and suggest a resolution.\")]\n",
    "# messages = [HumanMessage(content=\"The machine's vibration is 3.2, pressure is 45, temperature is 78.\")]\n",
    "messages = [HumanMessage(content=\"My machine is running very hot, I am wondering what the cuase is and how I can solve it.\")]\n",
    "response = pm_agent.invoke({\"messages\": messages,}, config)\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa63733-ace7-4743-b453-255d7729f82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# messages = response['messages'] + [HumanMessage(content=\"yes, pleasse\")]\n",
    "messages = [HumanMessage(content=\"yes, please\")]\n",
    "response = pm_agent.invoke({\"messages\": messages}, config)\n",
    "\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb0143df-9c51-41aa-a8df-0abda91beb0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log and Register Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "270fddf7-4ae4-45ea-9dc9-4e3c1812c92b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from pkg_resources import get_distribution\n",
    "from mlflow.models.signature import infer_signature\n",
    "from chat_agent import pm_agent\n",
    "\n",
    "input_example = {\"messages\": [{\"role\": \"user\", \"content\": \"how can you assist?\"}]}\n",
    "output_example = pm_agent.invoke(input_example)\n",
    "\n",
    "# signature = infer_signature(input_example, output_example)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"pm_agent\",\n",
    "        python_model=\"chat_agent.py\",\n",
    "        pip_requirements=[\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00db14b7-7fd7-4b83-a99c-1cd46f84d178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the agent\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"genai_demo\"\n",
    "AGENT_NAME = 'pm_chat_agent'\n",
    "AGENT_NAME_FULL = f\"{CATALOG}.{SCHEMA}.{AGENT_NAME}\"\n",
    "registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=AGENT_NAME_FULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92c34c3f-99c7-4f5e-b385-037dac9f2384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe94a06-6e07-45cd-9b17-00a1f50b97d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "AGENT_VERSION = 1\n",
    "agents.deploy(AGENT_NAME_FULL, AGENT_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdccfa9e-9a2f-4ec2-8dba-b22e9162b14f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b7bd347-7556-4ade-b6f3-82c73cffffaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "129dea31-8a47-4c89-9b71-9e841c5a3925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae69ce9-078c-4ebe-a3bd-52bc0121e857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_chat_agent_pm_rca",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
